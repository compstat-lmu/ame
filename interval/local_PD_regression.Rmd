---
title: Local Partial Dependence Regression example
author: 
  name: Bodo Burger
  affiliation: LMU Munich
date: 2018-07
output:
  html_document: 
    toc: yes
  github_document:
    toc: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE,
                      cache = TRUE, cache.path = "cache/",
                      fig.path = "figures/file-name-", fig.height = 4, fig.width = 5)
library("ggplot2")
library("mlr")
#library("ame")
devtools::load_all()

theme_set(theme_light())
set.seed(4218)

normalize = function(x, lower.bound = 0, upper.bound = 1) {
  x.min = min(x)
  c1 = (upper.bound - lower.bound)/(max(x) - x.min)
  c2 = lower.bound - c1 * x.min
  return(c1 * x + c2)
}
```

# Data generating process

```{r dgp,fig.width=6}
n = 200
x = runif(n, min = 0, max = 1)
x1 = x + rnorm(n, 0, 0.05)
x2 = x + rnorm(n, 0, 0.05)
x3 = normalize(0.9 * x + rnorm(n, 0, .1))
x4 = .85 * x + rnorm(n, 0, .1)
x5 = normalize(0.5 * x + rnorm(n, 0, .1))
y1 = function(x) x
y2 = function(x) 10*(x-.25)^2
y3 = function(x) -2 * x
y4 = function(x) 40 * ((x-.2)^3 - (x-.2)^2) + 4
y5 = function(x) -4 * cos(4*pi*x) * x + 4
x.grid = seq(0, 1, .01)
par(mfrow = c(2,3), oma = c(0, 0, 2, 0))
plot(x.grid, y1(x.grid), type = "l")
plot(x.grid, y2(x.grid), type = "l")
plot(x.grid, y3(x.grid), type = "l")
plot(x.grid, y4(x.grid), type = "l")
plot(x.grid, y5(x.grid), type = "l")
mtext("True partial effects", outer = TRUE, cex = 1.5)
y = y1(x1) + y2(x2) + y3(x3) + y4(x4) + y5(x5) + rnorm(n, 0, .1)
dt = data.frame(y, x1, x2, x3, x4, x5)
knitr::kable(cor(dt))
```

# Models

We try linear model, neural net, SVM and (to cover a tree method) gradient boosting.

```{r fit-models}
tsk = makeRegrTask(data = dt, target = "y")
# linear model:
lm.lrn = makeLearner("regr.lm")
lm.mod = train(lm.lrn, tsk)
lm.mod$learner.model
# neural net:
nnet.lrn = makeLearner("regr.nnet", skip = FALSE, size = 20, decay = 0.0001, maxit = 1000,
  trace = FALSE)
nnet.mod = train(nnet.lrn, tsk)
# svm:
svm.lrn = makeLearner("regr.svm")
svm.mod = train(svm.lrn, tsk)
# tree method:
ntrees = 10000
gbm.lrn = makeLearner("regr.gbm", n.trees = ntrees, interaction.depth = 1, shrinkage = .02)
gbm.mod = train(gbm.lrn, tsk)
```

# LPD Linear Model

```{r,fig.width=9}
lm.lpd.x1 = computeLPD(lm.mod$learner.model, dt, "x1", l = 20)
p1 = plotLPD(lm.lpd.x1) + ggtitle("LPD, l=20")
lm.lpd.x1 = computeLPD(lm.mod$learner.model, dt, "x1", l = 180)
p2 = plotLPD(lm.lpd.x1) + ggtitle("LPD, l = 200")
p3 = plotPartialDependence(generatePartialDependenceData(lm.mod, tsk, "x1", n = 20,
  individual = FALSE)) + ggtitle("PD")
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

LPD does not produce a straight line with slope equal to the coefficient of the linear model for low l.
For *l = nrow(data)* LPD equals PD.

```{r}
l = 50
gridExtra::grid.arrange(
  plotLPD(computeLPD(lm.mod$learner.model, dt, "x2", l = l)),
  plotLPD(computeLPD(lm.mod$learner.model, dt, "x3", l = l)),
  plotLPD(computeLPD(lm.mod$learner.model, dt, "x4", l = l)),
  plotLPD(computeLPD(lm.mod$learner.model, dt, "x5", l = l)),
  ncol = 2)
```

# LPD neural net

```{r}
plotLPD(computeLPD(nnet.mod$learner.model, dt, "x2", n = 20, l = 120))

plotPartialDependence(generatePartialDependenceData(nnet.mod, tsk, "x5", n = 20))

```

